'''
This code snippet is designed to vectorize questions and their corresponding aids (yordam) from a text file using the SentenceTransformer model. It reads the text file, processes the content to extract questions and aids, generates embeddings for each entry, and saves the embeddings in a FAISS index along with metadata in a JSON file. The output is stored in a specified directory on Google Drive.
'''


import re, os, json
import numpy as np
from tqdm import tqdm
import faiss
from sentence_transformers import SentenceTransformer

# ğŸ”§ Model
model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

# ğŸ“ Yol tanÄ±mlarÄ±
txt_path = "/content/drive/MyDrive/rd_project/soru_yordam_listesi.txt"

# âœ… Google Drive Ã¼zerine yazÄ±lacak dizin
BASE_DIR = "/content/drive/MyDrive/rd_project"
output_dir = os.path.join(BASE_DIR, "faiss_outputs")
os.makedirs(output_dir, exist_ok=True)

# ğŸ“„ SORU + YORDAM'larÄ± oku
with open(txt_path, "r", encoding="utf-8") as f:
    content = f.read()

# ğŸ” Bloklara ayÄ±r
blocks = content.strip().split("-" * 30)

# ğŸ“¦ HazÄ±rlÄ±k
entries = []

for block in blocks:
    block = block.strip()
    if not block:
        continue

    match = re.search(r"SORU\s+(\d+):\s*(.*?)\nYORDAM\s+\1:\s*(.*)", block, re.DOTALL)
    if match:
        idx, soru, yordam = match.groups()
        soru = soru.strip().replace("\n", " ")
        yordam = yordam.strip().replace("\n", " ")

        # BoÅŸsa sadece soruyu al
        combined_text = f"SORU: {soru}" if "[BoÅŸ]" in yordam else f"SORU: {soru}\nYORDAM: {yordam}"

        entries.append({
            "id": int(idx),
            "soru": soru,
            "yordam": yordam if "[BoÅŸ]" not in yordam else "",
            "text": combined_text.strip()
        })

print(f"ğŸ” Toplam {len(entries)} soru-yordam Ã§ifti bulundu.")

# ğŸ§  Embedding Ã¼ret
texts = [entry["text"] for entry in entries]
embeddings = model.encode(texts, show_progress_bar=True, convert_to_numpy=True)

# ğŸ’¾ FAISS index oluÅŸtur
dimension = embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(embeddings)

# ğŸ“¤ Kaydet
faiss.write_index(index, os.path.join(output_dir, "faiss_soru_yordam.index"))
with open(os.path.join(output_dir, "metadata_soru_yordam.json"), "w", encoding="utf-8") as f:
    json.dump(entries, f, ensure_ascii=False, indent=2)

print(f"âœ… FAISS ve metadata baÅŸarÄ±yla Google Drive'a yazÄ±ldÄ± â†’ {output_dir}")
