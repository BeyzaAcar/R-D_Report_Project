'''
Bu script, PDF raporunu iÅŸleyerek nihai cevap Ã¼retir.
Raporu adÄ±m adÄ±m iÅŸler:
1. PDF'den metin Ã§Ä±karÄ±r.
2. CID temizler.
3. Metni cÃ¼mle cÃ¼mle chunk'lar.
4. Chunk'larÄ± FAISS ile indeksler.
5. Soruya gÃ¶re top-k chunk'larÄ± bulur.
#     out_dir = os.path.join(workspace_dir, "chunks")
6. Top-k chunk'larÄ± geniÅŸletir.
7. GPT ile nihai cevap oluÅŸturur.
8. Sonucu ekrana yazdÄ±rÄ±r.
#         raw_text = f.read()
'''

import os, argparse
from pathlib import Path
from dotenv import load_dotenv; load_dotenv() # .env dosyasÄ±nÄ± yÃ¼kledik burada

# burada .env dosyasÄ±ndan deÄŸiÅŸkenleri okuyoruz mesela root degiskenine env dosyasÄ±ndan gelen WORKSPACE_ROOT deÄŸerini atÄ±yoruz
ROOT        = Path(os.getenv("WORKSPACE_ROOT", "workspace")) # eger .env yoksa varsayÄ±lan "workspace"
EMBED_MODEL = os.getenv("EMBED_MODEL", "sentence-transformers/all-MiniLM-L6-v2") # varsayÄ±lan model
TOPK       = int(os.getenv("TOPK", 10)) # varsayÄ±lan top-k sayÄ±sÄ±


# ---- modÃ¼ller ----
import init_workspace, pdf_to_text, cid_cleaner
import chunk_creator, faiss_creator, soru_yordam_embedder
import search_faiss_top_chunks, expand_top10_chunks
import gpt_amacalismiyor       # hÃ¢lÃ¢ â€œstubâ€ durumda

def main(pdf_path: str, question_id: int):
    print("BASLÄ°YORUMMMMM")

    
    print(f"ğŸ“„ PDF: {pdf_path}")
    # 1) Workspace
    init_workspace.init_workspace(ws)

    # 1.5) SorularÄ± FAISS'e embedle
    txt_path = "QUESTIONS/default_questions_and_yordams.txt"
    soru_yordam_embedder.vectorize_soru_yordam(txt_path, str(ws), EMBED_MODEL)

    print(f"ğŸ“‚ Workspace: {ws}")
    # 2) OCR / PDF to text
    raw_txt = pdf_to_text.pdf_to_txt(pdf_path, ws)

    print(f"ğŸ“„ Raw text extracted: {len(raw_txt)} characters")
    # 3) CID temizle
    clean_txt = cid_cleaner.clean_txt(raw_txt, ws)

    print(f"ğŸ“„ Cleaned text: {len(clean_txt)} characters")
    # 4) Chunk
    chunk_creator.create_chunks(clean_txt, ws)

    print(f"ğŸ“‚ Chunks created: {len(os.listdir(ws / 'chunks'))} files")
    # 5) FAISS
    faiss_creator.create_faiss_for_chunks(ws, model_name=EMBED_MODEL)

    print("ğŸ” FAISS index created")
    # 6) Top-k
    search_faiss_top_chunks.ask_all(ws, top_k=TOPK, model_name=EMBED_MODEL)

    print(f"ğŸ”Ÿ Top-10 chunks found for question ID {question_id}")
    # 7) GeniÅŸlet
    expand_top10_chunks.expand_chunk(ws)

    print("ğŸ“ˆ Expanded top-10 chunks saved")
    # 8) LLM cevap (placeholder)
    answer = gpt_amacalismiyor.generate_prompt(question_id, ws)
    print("\nğŸŸ¢ FINAL ANSWER\n", answer)

if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("report_id", help="Ã–rn: rapor2023")
    ap.add_argument("pdf", help="PDF dosyasÄ±nÄ±n yolu")
    ap.add_argument("qid", type=int, help="Soru IDâ€™si (Ã¶rn: 1)")
    args = ap.parse_args()

    REPORT_ID = args.report_id
    ws = ROOT / REPORT_ID
    main(args.pdf, args.qid)
